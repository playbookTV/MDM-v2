Perfect. Let‚Äôs treat this properly: I‚Äôll rewrite the Product Requirements Document (PRD) and Technical Requirements Document (TRD) as two separate, detailed, and extensive documents‚Äîbut scoped for a React-based web curation app (instead of a Python TUI).

‚∏ª

üìÑ Product Requirements Document (PRD)

Product: Modomo Dataset Curation Web App
Version: v1.0
Owner: Modomo Core Team

‚∏ª

1. Overview

The Modomo Dataset Curation Web App is a browser-based interface for collecting, processing, and curating datasets of interior scenes. It enables ingestion of datasets (local or HuggingFace), AI-powered processing (scene classification, style classification, object detection, segmentation, depth estimation, material detection, and object description), and human review. All processed data is stored in Cloudflare R2 and Postgres for downstream training and inference.

Unlike the earlier Python TUI, this web app is designed for scalability, collaboration, and extensibility, providing a clear upgrade path into Modomo‚Äôs long-term Review & Annotation Dashboard.

‚∏ª

2. Goals
	‚Ä¢	Ingest and process large-scale datasets of room images.
	‚Ä¢	Provide a rich, modern UI for visualizing results (scenes, objects, masks, depth maps, metadata).
	‚Ä¢	Enable filtering and distribution insights (scene type, style, materials).
	‚Ä¢	Allow human-in-the-loop review (approve/reject, edit labels).
	‚Ä¢	Persist results consistently to R2 (artifacts) and Postgres (metadata).
	‚Ä¢	Create a foundation for Modomo‚Äôs data governance pipeline.

‚∏ª

3. Non-Goals
	‚Ä¢	No training orchestration (handled by central server).
	‚Ä¢	No real-time collaborative editing (v1 is single-user or sequential).
	‚Ä¢	No payment/auth flows (internal tool only for now).

‚∏ª

4. User Stories
	1.	As a data curator, I can upload a folder of images or pull from a HuggingFace dataset so I can quickly assemble a new dataset.
	2.	As a reviewer, I can browse a scene, see detected objects with masks and metadata, and approve/reject them to ensure dataset quality.
	3.	As an engineer, I can track dataset processing jobs, see logs/errors, and verify that results are stored correctly.
	4.	As a designer, I can view distribution dashboards (scenes, styles, materials) to identify dataset imbalances.
	5.	As a researcher, I can search for specific categories (e.g., ‚Äúsofas with fabric material‚Äù) to build targeted subsets.

‚∏ª

5. Features

Ingestion
	‚Ä¢	Upload local folder of images (drag-and-drop).
	‚Ä¢	Pull dataset from HuggingFace URL.
	‚Ä¢	Preview dataset before processing.

Processing
	‚Ä¢	Launch pipeline (via backend workers).
	‚Ä¢	AI stages: Scene Classifier, Style Classifier, YOLO, SAM2, GroundingDINO, DepthAnything, Material Detector, Object Description Generator, Color Palette Extractor.
	‚Ä¢	Display progress + logs.

Review
	‚Ä¢	Scene viewer: original + overlays (depth, masks, bounding boxes).
	‚Ä¢	Object panel: category, materials, description, attributes.
	‚Ä¢	Approve/Reject/Edit per object.
	‚Ä¢	Export approved subsets.

Dashboard
	‚Ä¢	Stats overview: scenes processed, objects detected, avg conf, failures.
	‚Ä¢	Distribution charts: scene types, styles, materials.
	‚Ä¢	Coverage metrics: description coverage, material detection coverage.

Persistence
	‚Ä¢	Store artifacts in R2 (images, masks, depth, thumbs).
	‚Ä¢	Store structured metadata in Postgres.

‚∏ª

6. Success Metrics
	‚Ä¢	‚â•95% images processed successfully.
	‚Ä¢	Dataset imbalance detection: top-5 scene types/styles displayed.
	‚Ä¢	Reviewer throughput ‚â• 100 scenes/hour.
	‚Ä¢	Latency: job pipeline feedback ‚â§ 5s per scene in UI.

‚∏ª

7. Constraints & Assumptions
	‚Ä¢	Frontend: React (Vite + Tailwind + ShadCN + Radix).
	‚Ä¢	Backend: FastAPI or Express.
	‚Ä¢	Cloud storage: Cloudflare R2.
	‚Ä¢	Database: Postgres (Neon/Supabase).
	‚Ä¢	AI workers: Python microservices (on Runpod/VMs).
	‚Ä¢	Internal tool (no external auth in v1).

‚∏ª

8. Roadmap

Phase 1 (MVP): Ingest, process, persist, show stats.
Phase 2: Review UI with object overlays and approval workflow.
Phase 3: Distribution dashboards and filters.
Phase 4: Export tools + golden set evaluation.

‚∏ª

‚∏ª

üõ†Ô∏è Technical Requirements Document (TRD)

Product: Modomo Dataset Curation Web App
Version: v1.0

‚∏ª

1. Architecture

[Frontend: React] ‚îÄ‚îÄ‚îÄ‚Üí [API Gateway: FastAPI/Express] ‚îÄ‚îÄ‚îÄ‚Üí [Workers: Python ML pipeline]
         ‚îÇ                       ‚îÇ                                ‚îÇ
         ‚îÇ                       ‚îÇ                                ‚îú‚îÄ‚îÄ YOLO, SAM2, GroundingDINO
         ‚îÇ                       ‚îÇ                                ‚îú‚îÄ‚îÄ DepthAnything, CLIP
         ‚îÇ                       ‚îÇ                                ‚îî‚îÄ‚îÄ Captioner
         ‚îÇ                       ‚îî‚îÄ‚îÄ‚Üí [Postgres (metadata)]
         ‚îî‚îÄ‚îÄ‚Üí [Cloudflare R2 (artifacts)]


‚∏ª

2. Frontend (React)
	‚Ä¢	Framework: React + Vite (Preact optional for lighter footprint).
	‚Ä¢	Styling/UI: Tailwind CSS, ShadCN components, Radix primitives.
	‚Ä¢	State Management: Zustand or React Query for data fetching.
	‚Ä¢	Charts: Recharts or Chart.js for distribution graphs.
	‚Ä¢	File Upload: React Dropzone.
	‚Ä¢	Views:
	‚Ä¢	Dataset Explorer (upload/URL input ‚Üí list of scenes).
	‚Ä¢	Job Queue Panel (job list, status, logs).
	‚Ä¢	Scene Browser (image viewer with overlays: masks, bboxes, depth).
	‚Ä¢	Object Inspector (category, materials, desc, attributes).
	‚Ä¢	Stats Dashboard (charts).

‚∏ª

3. Backend (API Gateway)
	‚Ä¢	Language: Python (FastAPI) or Node.js (Express).
	‚Ä¢	Endpoints:
	‚Ä¢	POST /datasets ‚Üí register dataset (name, source_url).
	‚Ä¢	POST /jobs ‚Üí create processing job (dataset_id).
	‚Ä¢	GET /jobs/:id ‚Üí job status, logs.
	‚Ä¢	GET /stats ‚Üí aggregated distributions (scenes, styles, materials).
	‚Ä¢	GET /scenes/:id ‚Üí scene details + objects.
	‚Ä¢	PATCH /objects/:id ‚Üí update verdict/labels.
	‚Ä¢	Queue: Redis-backed task queue for workers.
	‚Ä¢	Auth: JWT or API key (internal).

‚∏ª

4. Workers (ML Pipeline)
	‚Ä¢	Frameworks: PyTorch + HuggingFace + Ultralytics + OpenCV.
	‚Ä¢	Stages (modular):
	‚Ä¢	Scene Classifier ‚Üí Places365/CLIP prototypes.
	‚Ä¢	Style Classifier ‚Üí CLIP prototype-based multi-label.
	‚Ä¢	YOLO ‚Üí detections.
	‚Ä¢	SAM2 ‚Üí segmentation masks.
	‚Ä¢	GroundingDINO ‚Üí category labels.
	‚Ä¢	DepthAnything ‚Üí depth maps.
	‚Ä¢	CLIP-based Material Detector.
	‚Ä¢	Captioner (BLIP-like) ‚Üí descriptions.
	‚Ä¢	Color extractor (k-means on LAB).
	‚Ä¢	Output: JSON blob per scene (objects, labels, materials, descs).
	‚Ä¢	Persistence: Upload artifacts to R2, write rows to DB.

‚∏ª

5. Database Schema

Core tables: datasets, jobs, scenes, scene_styles, objects, object_materials, reviews.
	‚Ä¢	Already detailed in our schema conversation.
	‚Ä¢	Ensure indexes on: scene_type, style_code, material_code, category_code.

‚∏ª

6. Storage (R2)
	‚Ä¢	Bucket: modomo-datasets
	‚Ä¢	Key convention:

scenes/{scene_id}.jpg
depth/{scene_id}.png
masks/{scene_id}/{object_id}.png
thumbs/{scene_id}/{object_id}.jpg


	‚Ä¢	Metadata persisted in DB.

‚∏ª

7. Observability
	‚Ä¢	Frontend: Sentry for error tracking, LogRocket for session replay.
	‚Ä¢	Backend: Structured logs (JSON), Prometheus metrics, Grafana dashboards.
	‚Ä¢	Workers: Job logs ‚Üí Redis stream or DB job_events.

‚∏ª

8. Performance Considerations
	‚Ä¢	Batch inference on workers (multiple crops per forward).
	‚Ä¢	Resize images to ‚â§1024px before processing to control VRAM.
	‚Ä¢	Parallelize jobs with worker pool (Runpod pods).
	‚Ä¢	UI paginates results (50 scenes/page).

‚∏ª

9. Security
	‚Ä¢	Signed URLs for R2 uploads (short TTL).
	‚Ä¢	Row-Level Security in Supabase if multi-user.
	‚Ä¢	No PII stored (images are interiors only).

‚∏ª

10. Roadmap
	‚Ä¢	MVP: ingest + process pipeline + stats dashboard.
	‚Ä¢	R1: review interface (approve/reject/edit).
	‚Ä¢	R2: advanced dashboards (imbalance detection, filters).
	‚Ä¢	R3: export subsets + training-ready dataset builder.

‚∏ª

‚úÖ With this split:
	‚Ä¢	PRD = what we‚Äôre building, why, who for, success criteria.
	‚Ä¢	TRD = how we‚Äôre building it, tech stack, architecture, schema, APIs, infra.

‚∏ª

