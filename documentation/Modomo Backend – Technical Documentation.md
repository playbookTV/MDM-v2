Perfect â€” letâ€™s lock in a Technical Documentation for your backend stack (Railway FE/API + Redis, Runpod GPU worker, Supabase DB, Cloudflare R2). This will read like an internal dev handbook so you (and teammates) can spin it up, maintain it, and extend it without guesswork.

â¸»

ðŸ“˜ Modomo Backend â€“ Technical Documentation

Version: v1.0
Stack:
	â€¢	Frontend: React (served via Railway, optional Vercel)
	â€¢	API/Orchestrator: FastAPI (Python) or Express (Node.js) on Railway
	â€¢	Queue: Redis (Railway add-on)
	â€¢	GPU Pipeline: Python worker(s) on Runpod
	â€¢	Storage: Cloudflare R2
	â€¢	Database: Supabase (Postgres)

â¸»

1. System Architecture

 [React FE] â”€â”€â”€â†’ [Railway API/Orchestrator] â”€â”€â”€â†’ [Supabase (DB)]
        â”‚                     â”‚
        â”‚                     â”œâ”€â”€â†’ [Redis Queue] (Railway)
        â”‚                     â”‚
        â”‚                     â””â”€â”€â†’ [Cloudflare R2] (presign)
        â”‚
        â–¼
 [Scene Review UI]            [Runpod GPU Worker(s)]
                                    â”‚
                                    â”œâ”€â”€â†’ Download originals (R2)
                                    â”œâ”€â”€â†’ Run pipeline (YOLO, SAM2, DINO, Depth, CLIP, Caption)
                                    â”œâ”€â”€â†’ Upload masks/depth/thumbs (R2)
                                    â””â”€â”€â†’ Write metadata (Supabase)


â¸»

2. Core Services

2.1 API / Orchestrator (Railway)
	â€¢	Responsibilities:
	â€¢	Dataset registration & scene ingestion
	â€¢	R2 presign for uploads/reads
	â€¢	Job creation & orchestration (enqueue per-scene tasks)
	â€¢	SSE endpoint for live job updates
	â€¢	Read endpoints for FE (datasets, jobs, scenes, objects, stats)
	â€¢	Write endpoints for annotations/reviews
	â€¢	Framework: FastAPI (Python) or Express (Node.js)
	â€¢	Port: 8080
	â€¢	Healthcheck: /health returns { "ok": true }

Endpoints (highlights):
	â€¢	POST /datasets, POST /datasets/:id/presign, POST /datasets/:id/register-scenes
	â€¢	POST /jobs (creates rows + enqueues per-scene messages)
	â€¢	GET /jobs, GET /jobs/:id, GET /jobs/:id/events (SSE)
	â€¢	GET /datasets/:id/scenes, GET /scenes/:id
	â€¢	PATCH /objects/:id, POST /reviews
	â€¢	GET /stats/overview, GET /stats/distribution/:kind

â¸»

2.2 Queue (Redis on Railway)
	â€¢	Namespace: modomo:*
	â€¢	Keys:
	â€¢	modomo:jobs:queue â†’ main list (scene jobs)
	â€¢	modomo:jobs:events:<job_id> â†’ stream of events (progress logs)
	â€¢	modomo:locks:<scene_id> â†’ processing lock (idempotency)
	â€¢	modomo:jobs:dead â†’ dead-letter queue
	â€¢	Message Shape (per-scene job):

{
  "job_id": "uuid",
  "scene_id": "uuid",
  "r2_key_original": "scenes/<id>.jpg",
  "resize_max_px": 1280,
  "stages": {
    "scene": true, "style": true, "detect": true,
    "segment": true, "label": true, "depth": true,
    "material": true, "caption": true, "palette": true
  }
}


â¸»

2.3 GPU Worker (Runpod)
	â€¢	Responsibilities:
	â€¢	Consume scene jobs from Redis
	â€¢	Preload models (YOLO, SAM2, GroundingDINO, DepthAnything, CLIP, captioner)
	â€¢	Download original â†’ run pipeline â†’ upload artifacts (R2)
	â€¢	Persist metadata (Supabase)
	â€¢	Emit job events back to Redis
	â€¢	Concurrency:
	â€¢	1 job at a time on 24GB
	â€¢	2 jobs on 48GB if all models fit simultaneously
	â€¢	Endpoints (internal):
	â€¢	GET /health â†’ { "ok": true, "gpu_mem_used": â€¦ }
	â€¢	POST /warmup â†’ loads models into VRAM
	â€¢	Events emitted (examples):

{"type":"warming","job_id":"uuid"}
{"type":"scene_start","scene_id":"uuid"}
{"type":"scene_done","scene_id":"uuid","objects":7,"avg_conf":0.82}
{"type":"scene_failed","scene_id":"uuid","error":"CUDA OOM"}


â¸»

2.4 Storage (Cloudflare R2)
	â€¢	Bucket: modomo-datasets
	â€¢	Prefixes:
	â€¢	scenes/{scene_id}.jpg
	â€¢	depth/{scene_id}.png
	â€¢	masks/{scene_id}/{object_id}.png
	â€¢	thumbs/{scene_id}/{object_id}.jpg
	â€¢	Access:
	â€¢	API presigns PUT URLs for FE uploads
	â€¢	API presigns GET URLs for FE viewing
	â€¢	Worker uses R2 credentials directly

â¸»

2.5 Database (Supabase/Postgres)
	â€¢	Schema (core):
	â€¢	datasets â†’ dataset metadata
	â€¢	jobs â†’ job state + progress
	â€¢	scenes â†’ scene rows (type, styles, palette, depth)
	â€¢	scene_styles â†’ multi-label style assignments
	â€¢	objects â†’ detected objects per scene
	â€¢	object_materials â†’ multi-label materials
	â€¢	reviews â†’ human corrections/verdicts
	â€¢	Indexes:
	â€¢	scene_type, style_code, material_code, category_code
	â€¢	objects(confidence)
	â€¢	GIN on JSON fields (attrs, palette)

â¸»

3. Pipeline Overview (Worker)

Stages:
	1.	Scene Classifier (Places365 / CLIP prototype)
	2.	Style Classifier (CLIP prototype, multi-label)
	3.	YOLOv8 (object detection)
	4.	SAM2 (segmentation)
	5.	GroundingDINO (object categories)
	6.	DepthAnything (depth maps)
	7.	CLIP-based Material Detector
	8.	Captioner (BLIP-like) â†’ short description
	9.	Color Extractor (k-means LAB) â†’ scene palette

Outputs:
	â€¢	Depth map, masks, thumbnails â†’ R2
	â€¢	Scene row â†’ Supabase
	â€¢	Object rows + materials + attrs â†’ Supabase

â¸»

4. Job Lifecycle
	1.	FE uploads dataset â†’ API registers scenes in DB.
	2.	POST /jobs â†’ API creates job row (status queued), enqueues N scene messages.
	3.	Worker pulls job â†’ emits warming if first run.
	4.	Worker processes scene â†’ uploads artifacts, writes to DB.
	5.	Worker emits scene_done event â†’ API streams via SSE to FE.
	6.	When all scenes done, API marks job as succeeded.
	7.	Failures â†’ retried twice â†’ dead-letter if still failing.

â¸»

5. Error Handling
	â€¢	Retries: 2 retries per scene job (15s â†’ 60s backoff).
	â€¢	Dead letters: failed jobs written to modomo:jobs:dead.
	â€¢	Idempotency: locks prevent double-processing same scene.
	â€¢	Partial saves: if some stages succeed, DB + R2 writes still kept (status partial).
	â€¢	User feedback: SSE emits scene_failed; FE dashboard shows red badge.

â¸»

6. Deployment & Infra

Railway (FE + API + Redis)
	â€¢	Frontend:
	â€¢	Build React â†’ static bundle â†’ Railway Nginx or Express static serve.
	â€¢	API:
	â€¢	Deploy Dockerized FastAPI/Express app.
	â€¢	Health: /health returns 200.
	â€¢	Redis:
	â€¢	Use Railway add-on.
	â€¢	Connect via REDIS_URL.

Runpod (GPU Worker)
	â€¢	Dockerfile includes: torch, ultralytics, groundingdino, sam2, depthanything, CLIP, BLIP, boto3, asyncpg.
	â€¢	Startup script:
	â€¢	Run /warmup on start.
	â€¢	Connect to Redis, block on modomo:jobs:queue.
	â€¢	Autosleep: 10â€“15min idle timeout.

Supabase (Postgres)
	â€¢	Use Supabase-managed Postgres.
	â€¢	Enable sslmode=require.
	â€¢	Optional Row-Level Security if multi-user.

Cloudflare R2
	â€¢	Public bucket: off.
	â€¢	Access only via presigned URLs.
	â€¢	Objects lifecycle rules (archive/delete after N months if desired).

â¸»

7. Observability
	â€¢	Logs:
	â€¢	API logs structured JSON (job_id, scene_id, stage, latency).
	â€¢	Worker logs per scene job with durations + errors.
	â€¢	Metrics:
	â€¢	Jobs queued, in-flight, succeeded, failed.
	â€¢	GPU memory usage, stage latency (p50/p95).
	â€¢	Dashboards: Grafana/Prometheus (if desired).
	â€¢	Alerts: job failure rate > 5% over 10min.

â¸»

8. Security
	â€¢	API: restrict CORS to FE domain.
	â€¢	R2: always presigned URLs (5â€“15min TTL).
	â€¢	Redis: require password, restrict IPs to Railway + Runpod.
	â€¢	Supabase: connect with SSL only.
	â€¢	Auth: JWT (internal), optional API keys for external integrations.

â¸»

9. Scaling & Cost Control
	â€¢	GPU concurrency: 1â€“2 concurrent scenes per worker depending on VRAM.
	â€¢	Horizontal scale: spawn N Runpod workers consuming same Redis queue.
	â€¢	Autosleep: Runpod pod autoscales to 0 when idle.
	â€¢	Railway scale: API scales vertically (CPU/mem) but no GPU; cheap to run.
	â€¢	Batching: process multiple crops in one CLIP forward to save time.
	â€¢	Image size: resize â‰¤ 1280px.

â¸»

10. Developer Workflow

Testing
	â€¢	Unit tests for API (pytest/Jest).
	â€¢	Integration: spin up stub worker to test queue â†’ DB flow.
	â€¢	Golden set evaluation: 200 labeled scenes â†’ accuracy metrics.

CI/CD
	â€¢	GitHub Actions: lint, test, build Docker, deploy to Railway/Runpod.
	â€¢	Tag releases by semver.

