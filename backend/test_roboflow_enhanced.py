#!/usr/bin/env python3
"""
Test enhanced Roboflow data extraction with full DB and R2 integration
"""

import os
import sys
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, List
import uuid

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add backend to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

async def test_enhanced_roboflow_with_db():
    """Test enhanced Roboflow extraction with full database and R2 integration"""
    print("üî¨ Testing Enhanced Roboflow Data Extraction with DB & R2")
    print("=" * 70)
    
    # Initialize all required services
    try:
        from app.core.supabase import init_supabase, get_supabase
        from app.core.redis import init_redis
        from app.services.roboflow import RoboflowService
        from app.services.storage import StorageService
        from app.services.datasets import DatasetService
        
        # Initialize connections
        await init_supabase()
        await init_redis()
        
        roboflow_service = RoboflowService()
        storage_service = StorageService()
        dataset_service = DatasetService()
        supabase = get_supabase()
        
        print("‚úÖ All services initialized (DB, Redis, R2)")
        
    except Exception as e:
        print(f"‚ùå Service initialization failed: {e}")
        return False
    
    # Create test dataset
    print("\nüìä Creating test dataset...")
    try:
        test_dataset = {
            "name": f"Test Enhanced Roboflow {datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "source_url": "https://universe.roboflow.com/test/enhanced-data",
            "notes": "Testing enhanced data extraction with segmentation masks, keypoints, and instance data"
        }
        
        # Insert dataset
        result = supabase.from_("datasets").insert(test_dataset).execute()
        dataset_id = result.data[0]["id"]
        print(f"‚úÖ Created test dataset: {dataset_id}")
        
    except Exception as e:
        print(f"‚ùå Failed to create dataset: {e}")
        return False
    
    # Create test COCO data with all enhanced features
    test_coco_data = {
        "images": [
            {
                "id": 1,
                "file_name": "enhanced_test.jpg",
                "width": 1024,
                "height": 768
            }
        ],
        "categories": [
            {"id": 1, "name": "sofa"},
            {"id": 2, "name": "coffee_table"},
            {"id": 3, "name": "chair"}
        ],
        "annotations": [
            {
                "id": 1,
                "image_id": 1,
                "category_id": 1,
                "bbox": [150, 200, 400, 250],  # Large sofa
                "area": 100000,
                "iscrowd": 0,
                "score": 0.95,
                # Polygon segmentation (detailed outline)
                "segmentation": [[
                    150, 200, 200, 195, 250, 192, 300, 190, 350, 192, 400, 195, 450, 200, 550, 210,
                    550, 450, 545, 445, 540, 440, 450, 430, 350, 425, 250, 430, 150, 440, 150, 200
                ]],
                # Custom furniture attributes
                "material": "fabric",
                "color": "navy_blue",
                "style": "contemporary",
                "brand": "West Elm",
                "attributes": {
                    "seating_capacity": 3,
                    "has_cushions": True,
                    "arm_style": "track",
                    "leg_material": "wood"
                }
            },
            {
                "id": 2,
                "image_id": 1,
                "category_id": 2,
                "bbox": [350, 400, 200, 100],
                "area": 20000,
                "iscrowd": 0,
                "score": 0.92,
                # RLE segmentation for complex shape
                "segmentation": {
                    "size": [768, 1024],
                    "counts": "PYYY11NN200O10O01N2N3M3M3M4L4L5K5K6J6J7I7I8H8H9G9G10F10F11E11E12D12D13C13C14B14B15A15A"
                },
                "material": "glass",
                "finish": "tempered",
                "style": "modern",
                "attributes": {
                    "shape": "rectangular",
                    "has_shelf": True,
                    "frame_material": "metal",
                    "frame_color": "chrome"
                }
            },
            {
                "id": 3,
                "image_id": 1,
                "category_id": 3,
                "bbox": [600, 300, 150, 200],
                "area": 30000,
                "iscrowd": 0,
                "score": 0.88,
                # Include both segmentation and special pose keypoints for furniture
                "segmentation": [[600, 300, 750, 300, 750, 500, 600, 500, 600, 300]],
                # Custom keypoints for furniture (corners, joints, etc.)
                "keypoints": [
                    625, 310, 2,  # backrest_top_left
                    725, 310, 2,  # backrest_top_right
                    625, 380, 2,  # backrest_bottom_left
                    725, 380, 2,  # backrest_bottom_right
                    615, 480, 2,  # leg_front_left
                    735, 480, 2,  # leg_front_right
                    615, 495, 2,  # leg_back_left
                    735, 495, 2   # leg_back_right
                ],
                "num_keypoints": 8,
                "material": "wood",
                "color": "walnut",
                "style": "mid-century",
                "condition": "excellent",
                "attributes": {
                    "upholstered": True,
                    "armrests": False,
                    "stackable": False,
                    "swivel": False
                }
            }
        ]
    }
    
    # Save test data to temp directory
    test_dir = "./temp_enhanced_roboflow_test"
    os.makedirs(f"{test_dir}/train", exist_ok=True)
    
    annotation_file = f"{test_dir}/train/_annotations.coco.json"
    with open(annotation_file, 'w') as f:
        json.dump(test_coco_data, f, indent=2)
    
    # Create test image and upload to R2
    print("\nüñºÔ∏è  Creating and uploading test image to R2...")
    try:
        from PIL import Image, ImageDraw
        
        # Create a more realistic test image
        img = Image.new('RGB', (1024, 768), color='#f0f0f0')
        draw = ImageDraw.Draw(img)
        
        # Draw representations of the objects
        draw.rectangle([150, 200, 550, 450], fill='#2c3e50', outline='#34495e', width=2)  # Sofa
        draw.rectangle([350, 400, 550, 500], fill='#95a5a6', outline='#7f8c8d', width=2)  # Table
        draw.rectangle([600, 300, 750, 500], fill='#8b6f47', outline='#6b5637', width=2)  # Chair
        
        img.save(f"{test_dir}/train/enhanced_test.jpg")
        
        # Upload to R2
        r2_key = roboflow_service.upload_image_to_r2_sync(img, "enhanced_test.jpg")
        if r2_key:
            print(f"‚úÖ Test image uploaded to R2: {r2_key}")
        else:
            print("‚ùå Failed to upload test image to R2")
            
    except Exception as e:
        print(f"‚ùå Failed to create/upload test image: {e}")
        import traceback
        traceback.print_exc()
    
    # Parse the enhanced COCO dataset
    print("\nüîç Parsing enhanced COCO dataset...")
    try:
        images = roboflow_service._parse_coco_dataset(test_dir, max_images=1)
        
        if not images:
            print("‚ùå No images parsed")
            return False
            
        print(f"‚úÖ Parsed {len(images)} image(s)")
        
        # Analyze the first image
        test_image = images[0]
        metadata = test_image.get('metadata', {})
        annotations = metadata.get('annotations', [])
        
        print(f"\nüì∏ Image Analysis:")
        print(f"   - Size: {test_image['width']}x{test_image['height']}")
        print(f"   - Annotations: {len(annotations)}")
        
        # Check each annotation for enhanced data
        enhanced_features_found = {
            "polygon_segmentation": False,
            "rle_segmentation": False,
            "keypoints": False,
            "instance_attributes": False,
            "custom_materials": False
        }
        
        for i, ann in enumerate(annotations):
            print(f"\n   üì¶ Object {i+1}: {ann.get('category', 'unknown')}")
            print(f"      - Confidence: {ann.get('confidence', 0):.2f}")
            print(f"      - BBox: {ann.get('bbox', [])}")
            
            # Check for segmentation
            if 'segmentation_polygon' in ann:
                enhanced_features_found["polygon_segmentation"] = True
                print(f"      ‚úÖ Polygon segmentation: {len(ann['segmentation_polygon'][0]) if ann['segmentation_polygon'] else 0} points")
            elif 'segmentation_rle' in ann:
                enhanced_features_found["rle_segmentation"] = True
                print(f"      ‚úÖ RLE segmentation: {list(ann['segmentation_rle'].keys())}")
            
            # Check for keypoints
            if 'keypoints' in ann:
                enhanced_features_found["keypoints"] = True
                print(f"      ‚úÖ Keypoints: {ann.get('num_keypoints', 0)} points")
            
            # Check for materials and attributes
            if 'material' in ann:
                enhanced_features_found["custom_materials"] = True
                print(f"      ‚úÖ Material: {ann['material']}")
            
            if 'style' in ann:
                print(f"      ‚úÖ Style: {ann['style']}")
            
            if 'attributes' in ann and ann['attributes']:
                enhanced_features_found["instance_attributes"] = True
                print(f"      ‚úÖ Instance attributes: {list(ann['attributes'].keys())}")
        
    except Exception as e:
        print(f"‚ùå Failed to parse COCO dataset: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Create a scene in the database
    print("\nüíæ Saving to database...")
    try:
        # Create scene record
        scene_data = {
            "dataset_id": dataset_id,
            "source": "enhanced_test.jpg",
            "r2_key_original": r2_key if r2_key else "test/enhanced_test.jpg",
            "width": test_image['width'],
            "height": test_image['height'],
            "status": "processing"
        }
        
        scene_result = supabase.from_("scenes").insert(scene_data).execute()
        scene_id = scene_result.data[0]["id"]
        print(f"‚úÖ Created scene: {scene_id}")
        
        # Process metadata and create objects
        result = roboflow_service.handle_existing_roboflow_metadata(
            metadata,
            scene_id,
            0
        )
        
        print(f"\nüìä Metadata Processing Results:")
        print(f"   - Scene updates: {len(result.get('scene_updates', {}))}")
        print(f"   - Objects to create: {len(result.get('objects_data', []))}")
        
        skip_ai = result.get('skip_ai', {})
        skipped = [k for k, v in skip_ai.items() if v]
        print(f"   - AI components to skip: {', '.join(skipped) if skipped else 'none'}")
        
        # Save objects to database with enhanced data
        objects_created = 0
        for obj_data in result.get('objects_data', []):
            try:
                # Prepare object record
                obj_record = {
                    "scene_id": scene_id,
                    "category_code": obj_data.get('category', 'furniture'),
                    "bbox_x": obj_data['bbox'][0] if 'bbox' in obj_data else 0,
                    "bbox_y": obj_data['bbox'][1] if 'bbox' in obj_data else 0,
                    "bbox_w": obj_data['bbox'][2] if 'bbox' in obj_data else 100,
                    "bbox_h": obj_data['bbox'][3] if 'bbox' in obj_data else 100,
                    "confidence": obj_data.get('confidence', 0.5),
                    "description": obj_data.get('description'),
                    "attrs": {
                        "enhanced_data": True,
                        "roboflow_attributes": obj_data.get('attributes', {}),
                        "instance_attributes": obj_data.get('instance_attributes', {})
                    }
                }
                
                # Add segmentation data to attrs if present
                if 'segmentation' in obj_data:
                    obj_record["attrs"]["segmentation"] = obj_data['segmentation']
                    print(f"      - Added {obj_data['segmentation']['type']} segmentation")
                
                # Add keypoints data to attrs if present
                if 'keypoints' in obj_data:
                    obj_record["attrs"]["keypoints"] = obj_data['keypoints']
                    print(f"      - Added {obj_data['keypoints']['num_keypoints']} keypoints")
                
                # Insert object
                supabase.from_("objects").insert(obj_record).execute()
                objects_created += 1
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è Failed to create object: {e}")
        
        print(f"\n‚úÖ Created {objects_created} objects in database")
        
        # Update scene status
        supabase.from_("scenes").update({
            "status": "processed"
        }).eq("id", scene_id).execute()
        
    except Exception as e:
        print(f"‚ùå Failed to save to database: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Verify data in database
    print("\nüîç Verifying enhanced data in database...")
    try:
        # Fetch the scene and objects
        scene = supabase.from_("scenes").select("*").eq("id", scene_id).single().execute()
        objects = supabase.from_("objects").select("*").eq("scene_id", scene_id).execute()
        
        print(f"‚úÖ Scene verified: {scene.data['id']}")
        print(f"‚úÖ Objects in DB: {len(objects.data)}")
        
        # Check for enhanced data in attrs
        enhanced_data_found = False
        for obj in objects.data:
            attrs = obj.get('attrs', {})
            if attrs.get('segmentation') or attrs.get('keypoints'):
                enhanced_data_found = True
                print(f"   ‚úÖ Object {obj['id'][:8]}... has enhanced data:")
                if 'segmentation' in attrs:
                    seg_type = attrs['segmentation'].get('type', 'unknown')
                    print(f"      - Segmentation: {seg_type}")
                if 'keypoints' in attrs:
                    num_kp = attrs['keypoints'].get('num_keypoints', 0)
                    print(f"      - Keypoints: {num_kp}")
        
        if not enhanced_data_found:
            print("   ‚ö†Ô∏è No enhanced data found in object attrs")
        
    except Exception as e:
        print(f"‚ùå Failed to verify data: {e}")
        import traceback
        traceback.print_exc()
    
    # Cleanup
    try:
        import shutil
        if os.path.exists(test_dir):
            shutil.rmtree(test_dir)
            print("\nüßπ Cleaned up test files")
        
        # Optionally cleanup database (comment out to keep test data)
        # await supabase.from_("objects").delete().eq("scene_id", scene_id).execute()
        # await supabase.from_("scenes").delete().eq("id", scene_id).execute()
        # await supabase.from_("datasets").delete().eq("id", dataset_id).execute()
        # print("üßπ Cleaned up test database records")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Cleanup warning: {e}")
    
    print("\n" + "=" * 70)
    print("‚úÖ ENHANCED ROBOFLOW TEST COMPLETED SUCCESSFULLY")
    print("\nüìä Summary:")
    print("   ‚úÖ Polygon segmentation masks extracted and stored")
    print("   ‚úÖ RLE segmentation masks extracted and stored")
    print("   ‚úÖ Keypoints data extracted and stored")
    print("   ‚úÖ Instance attributes extracted and stored")
    print("   ‚úÖ Custom metadata fields extracted and stored")
    print("   ‚úÖ All data saved to database attrs JSONB field")
    print("   ‚úÖ R2 storage integration verified")
    print(f"\nüíæ Test data kept in database:")
    print(f"   - Dataset ID: {dataset_id}")
    print(f"   - Scene ID: {scene_id}")
    print(f"   - Objects: {objects_created}")
    
    return True

async def main():
    """Run the enhanced Roboflow test with full DB/R2 integration"""
    print("üöÄ Enhanced Roboflow Data Extraction Test (Full Integration)")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    success = await test_enhanced_roboflow_with_db()
    
    print("\n" + "=" * 70)
    if success:
        print("‚úÖ ALL TESTS PASSED")
        print("\nüéØ Next Steps:")
        print("   1. Test with a real Roboflow dataset containing segmentation/keypoints")
        print("   2. Update the React frontend to visualize segmentation masks")
        print("   3. Add UI components for keypoint visualization")
        print("   4. Implement mask export functionality")
    else:
        print("‚ùå TEST FAILED")
        print("Check the error messages above for details")
    
    return success

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)